{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title ## Complete Transformer Code (Gradient Accumulation + Manual Mixed Precision)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Section 1: Setup and Imports\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"--- Section 1: Setup and Imports ---\")\n",
        "# Install necessary libraries\n",
        "!pip install -q flax optax tiktoken einops"
      ],
      "metadata": {
        "id": "VdUO5oa1H4rr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a724e89-cc59-4ea1-dd9f-8b939672a73d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Section 1: Setup and Imports ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import flax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import tiktoken\n",
        "import functools\n",
        "from einops import rearrange\n",
        "import urllib.request\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Determine device\n",
        "try:\n",
        "    if jax.devices('gpu'):\n",
        "        device = jax.devices('gpu')[0]\n",
        "        print(\"Using GPU\")\n",
        "    else:\n",
        "        device = jax.devices('cpu')[0]\n",
        "        print(\"No GPU found, using CPU\")\n",
        "except Exception:\n",
        "    if jax.devices('gpu'):\n",
        "        device = jax.devices('gpu')[0]\n",
        "        print(\"Using GPU\")\n",
        "    else:\n",
        "        device = jax.devices('cpu')[0]\n",
        "        print(\"Using CPU\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upK-p8rt1V7X",
        "outputId": "4fe49290-05e7-4531-8274-3e3179aec26b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Section 2: Data Preparation\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Section 2: Data Preparation ---\")\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/\"\n",
        "       \"main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Downloading {file_path}...\")\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(f\"File {file_path} already exists.\")\n",
        "\n",
        "print(\"Reading data from disk into RAM...\")\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()\n",
        "print(f\"Loaded text data into RAM ({len(text_data)} characters)\")\n",
        "\n",
        "print(\"Initializing tokenizer...\")\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer.n_vocab\n",
        "print(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
        "\n",
        "print(\"Encoding text data into token IDs (in RAM)...\")\n",
        "encoded_text = tokenizer.encode(text_data)\n",
        "print(\"Converting token IDs to JAX array...\")\n",
        "encoded_text_jax = jnp.array(encoded_text, dtype=jnp.int32)\n",
        "print(f\"Encoded text stored as JAX array with shape: {encoded_text_jax.shape}\")\n",
        "\n",
        "print(\"Splitting data into train/validation sets...\")\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(encoded_text_jax))\n",
        "train_data = encoded_text_jax[:split_idx]\n",
        "val_data = encoded_text_jax[split_idx:]\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "\n",
        "def create_batches(data, batch_size, context_length, key):\n",
        "    num_sequences = len(data) - context_length\n",
        "    if num_sequences <= 0:\n",
        "        raise ValueError(\"Dataset is too small for the given context length.\")\n",
        "    idxs = jax.random.permutation(key, num_sequences)\n",
        "    num_batches = num_sequences // batch_size\n",
        "    for i in range(num_batches):\n",
        "        batch_idxs = idxs[i * batch_size:(i + 1) * batch_size]\n",
        "        x_batch = jnp.stack([data[idx : idx + context_length] for idx in batch_idxs])\n",
        "        y_batch = jnp.stack([data[idx + 1 : idx + context_length + 1] for idx in batch_idxs])\n",
        "        yield x_batch, y_batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i_Anooy1Y_X",
        "outputId": "8f5509b7-2e9d-43ba-a684-89bbf6c17545"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Section 2: Data Preparation ---\n",
            "File the-verdict.txt already exists.\n",
            "Reading data from disk into RAM...\n",
            "Loaded text data into RAM (20479 characters)\n",
            "Initializing tokenizer...\n",
            "Tokenizer vocabulary size: 50257\n",
            "Encoding text data into token IDs (in RAM)...\n",
            "Converting token IDs to JAX array...\n",
            "Encoded text stored as JAX array with shape: (5145,)\n",
            "Splitting data into train/validation sets...\n",
            "Training data shape: (4630,)\n",
            "Validation data shape: (515,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Section 3: Model Configuration\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Section 3: Model Configuration ---\")\n",
        "config = {\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"context_length\": 128,\n",
        "    \"emb_dim\": 128,\n",
        "    \"n_heads\": 8,\n",
        "    \"n_layers\": 6,\n",
        "    \"qkv_bias\": False,\n",
        "    \"batch_size\": 16,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"compute_dtype\": jnp.bfloat16,\n",
        "    \"param_dtype\": jnp.float32,\n",
        "    \"output_head_dtype\": jnp.float32,\n",
        "}\n",
        "effective_batch_size = config[\"batch_size\"] * config[\"gradient_accumulation_steps\"]\n",
        "print(f\"Micro-batch size: {config['batch_size']}\")\n",
        "print(f\"Gradient Accumulation Steps: {config['gradient_accumulation_steps']}\")\n",
        "print(f\"Effective Batch Size: {effective_batch_size}\")\n",
        "print(f\"Compute dtype: {config['compute_dtype']}\")\n",
        "print(f\"Parameter dtype: {config['param_dtype']}\")\n",
        "print(f\"Output Head dtype: {config['output_head_dtype']}\")\n",
        "\n",
        "compute_dtype = config[\"compute_dtype\"]\n",
        "param_dtype = config[\"param_dtype\"]\n",
        "output_head_dtype = config[\"output_head_dtype\"]\n",
        "\n",
        "data_key = random.PRNGKey(0)\n",
        "batch_generator_demo = create_batches(train_data, config[\"batch_size\"], config[\"context_length\"], data_key)\n",
        "x_example, y_example = next(batch_generator_demo)\n",
        "print(\"\\nExample Input Batch Shape:\", x_example.shape)\n",
        "print(\"Example Target Batch Shape:\", y_example.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLMXFQp91deM",
        "outputId": "2d960a61-b9b1-46cd-be9a-a0ef4761c296"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Section 3: Model Configuration ---\n",
            "Micro-batch size: 16\n",
            "Gradient Accumulation Steps: 8\n",
            "Effective Batch Size: 128\n",
            "Compute dtype: <class 'jax.numpy.bfloat16'>\n",
            "Parameter dtype: <class 'jax.numpy.float32'>\n",
            "Output Head dtype: <class 'jax.numpy.float32'>\n",
            "\n",
            "Example Input Batch Shape: (16, 128)\n",
            "Example Target Batch Shape: (16, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Section 4: Transformer Components\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Section 4: Transformer Components ---\")\n",
        "\n",
        "class TokenAndPositionalEmbedding(nn.Module):\n",
        "    vocab_size: int\n",
        "    embed_dim: int\n",
        "    context_length: int\n",
        "    compute_dtype: jnp.dtype = compute_dtype\n",
        "    param_dtype: jnp.dtype = param_dtype\n",
        "\n",
        "    def setup(self):\n",
        "        self.tok_emb = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_dim, param_dtype=self.param_dtype)\n",
        "        self.pos_emb = nn.Embed(num_embeddings=self.context_length, features=self.embed_dim, param_dtype=self.param_dtype)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        token_embeddings = self.tok_emb(x)\n",
        "        positions = jnp.arange(seq_len)\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "        combined_embeddings = token_embeddings + position_embeddings\n",
        "        return combined_embeddings.astype(self.compute_dtype)\n",
        "\n",
        "class MultiHeadCausalSelfAttention(nn.Module):\n",
        "    embed_dim: int\n",
        "    num_heads: int\n",
        "    use_bias: bool = False\n",
        "    compute_dtype: jnp.dtype = compute_dtype\n",
        "    param_dtype: jnp.dtype = param_dtype\n",
        "\n",
        "    def setup(self):\n",
        "        assert self.embed_dim % self.num_heads == 0, \"Embed dim must be divisible by num_heads\"\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.q_proj = nn.Dense(features=self.embed_dim, use_bias=self.use_bias, name=\"query_proj\", dtype=self.compute_dtype, param_dtype=self.param_dtype)\n",
        "        self.k_proj = nn.Dense(features=self.embed_dim, use_bias=self.use_bias, name=\"key_proj\", dtype=self.compute_dtype, param_dtype=self.param_dtype)\n",
        "        self.v_proj = nn.Dense(features=self.embed_dim, use_bias=self.use_bias, name=\"value_proj\", dtype=self.compute_dtype, param_dtype=self.param_dtype)\n",
        "        self.out_proj = nn.Dense(features=self.embed_dim, use_bias=self.use_bias, name=\"output_proj\", dtype=self.compute_dtype, param_dtype=self.param_dtype)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
        "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
        "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
        "\n",
        "        attn_scores = jnp.matmul(q, jnp.transpose(k, (0, 1, 3, 2)))\n",
        "        scale_factor = jnp.sqrt(self.head_dim).astype(self.compute_dtype)\n",
        "        attn_scores = attn_scores / scale_factor\n",
        "\n",
        "        mask = nn.make_causal_mask(jnp.ones((batch_size, seq_len)), dtype=jnp.bool_)\n",
        "        attn_scores = jnp.where(mask, attn_scores, -jnp.inf)\n",
        "\n",
        "        attn_weights = jax.nn.softmax(attn_scores.astype(jnp.float32), axis=-1)\n",
        "        attn_weights = attn_weights.astype(self.compute_dtype)\n",
        "\n",
        "        context_vec = jnp.matmul(attn_weights, v)\n",
        "        context_combined = rearrange(context_vec, 'b h s d -> b s (h d)')\n",
        "        output = self.out_proj(context_combined)\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    embed_dim: int\n",
        "    compute_dtype: jnp.dtype = compute_dtype\n",
        "    param_dtype: jnp.dtype = param_dtype\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        hidden_dim = 4 * self.embed_dim\n",
        "        x = nn.Dense(features=hidden_dim, dtype=self.compute_dtype, param_dtype=self.param_dtype)(x)\n",
        "        x = nn.gelu(x)\n",
        "        x = nn.Dense(features=self.embed_dim, dtype=self.compute_dtype, param_dtype=self.param_dtype)(x)\n",
        "        return x\n",
        "\n",
        "@nn.remat # Keep remat for memory saving\n",
        "class TransformerBlock(nn.Module):\n",
        "    embed_dim: int\n",
        "    num_heads: int\n",
        "    use_bias: bool\n",
        "    compute_dtype: jnp.dtype = compute_dtype\n",
        "    param_dtype: jnp.dtype = param_dtype\n",
        "\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        attn_input = nn.LayerNorm(epsilon=1e-5, param_dtype=self.param_dtype)(x)\n",
        "        attn_output = MultiHeadCausalSelfAttention(\n",
        "            embed_dim=self.embed_dim, num_heads=self.num_heads, use_bias=self.use_bias,\n",
        "            compute_dtype=self.compute_dtype, param_dtype=self.param_dtype\n",
        "        )(attn_input)\n",
        "        x = x + attn_output\n",
        "\n",
        "        ffn_input = nn.LayerNorm(epsilon=1e-5, param_dtype=self.param_dtype)(x)\n",
        "        ffn_output = FeedForward(\n",
        "            embed_dim=self.embed_dim,\n",
        "            compute_dtype=self.compute_dtype, param_dtype=self.param_dtype\n",
        "        )(ffn_input)\n",
        "        x = x + ffn_output\n",
        "        return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLyLtuVZ1jZh",
        "outputId": "9a4630c0-7c8c-4a73-efc3-48abc7f838c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Section 4: Transformer Components ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Section 5: GPT Model Architecture\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Section 5: GPT Model Architecture (Manual MP + Remat) ---\")\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"A Generative Pre-trained Transformer model.\"\"\"\n",
        "    vocab_size: int = config[\"vocab_size\"]        # Size of the vocabulary.\n",
        "    embed_dim: int = config[\"emb_dim\"]            # Dimension of the token embeddings.\n",
        "    context_length: int = config[\"context_length\"]  # Maximum sequence length.\n",
        "    num_heads: int = config[\"n_heads\"]            # Number of attention heads.\n",
        "    num_layers: int = config[\"n_layers\"]          # Number of transformer blocks.\n",
        "    use_bias: bool = config[\"qkv_bias\"]           # Whether to use bias in QKV projections.\n",
        "    compute_dtype: jnp.dtype = config[\"compute_dtype\"]  # Data type for computation.\n",
        "    param_dtype: jnp.dtype = config[\"param_dtype\"]    # Data type for parameters.\n",
        "    output_head_dtype: jnp.dtype = config[\"output_head_dtype\"] # Data type for the final output layer.\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, idx):\n",
        "        \"\"\"\n",
        "        Forward pass of the GPT model.\n",
        "\n",
        "        Args:\n",
        "            idx: Input tensor of token indices, shape (batch_size, sequence_length).\n",
        "\n",
        "        Returns:\n",
        "            Logits tensor, shape (batch_size, sequence_length, vocab_size).\n",
        "        \"\"\"\n",
        "        # Generate token and positional embeddings.\n",
        "        x = TokenAndPositionalEmbedding(\n",
        "            vocab_size=self.vocab_size, embed_dim=self.embed_dim, context_length=self.context_length,\n",
        "            compute_dtype=self.compute_dtype, param_dtype=self.param_dtype, name=\"embedding\"\n",
        "        )(idx)\n",
        "\n",
        "        # Pass the embeddings through the transformer blocks.\n",
        "        for i in range(self.num_layers):\n",
        "            # Apply a single transformer block.\n",
        "            x = TransformerBlock(\n",
        "                embed_dim=self.embed_dim, num_heads=self.num_heads, use_bias=self.use_bias,\n",
        "                compute_dtype=self.compute_dtype,\n",
        "                param_dtype=self.param_dtype, name=f\"transformer_block_{i}\"\n",
        "            )(x)\n",
        "\n",
        "        # Apply final layer normalization.\n",
        "        x = nn.LayerNorm(epsilon=1e-5, name=\"final_ln\", param_dtype=self.param_dtype)(x)\n",
        "\n",
        "        # Project the final hidden states to vocabulary size to get logits.\n",
        "        logits = nn.Dense(\n",
        "            features=self.vocab_size, use_bias=False, name=\"output_projection\",\n",
        "            dtype=self.compute_dtype, param_dtype=self.param_dtype\n",
        "        )(x)\n",
        "        # Cast logits to the specified output data type.\n",
        "        logits = logits.astype(self.output_head_dtype)\n",
        "        return logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rccwXGqG1lQ-",
        "outputId": "37cd446c-1c33-4a75-ca8b-7813529c5d8a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Section 5: GPT Model Architecture (Manual MP + Remat) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Section 6: Training Setup\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Section 6: Training Setup (Manual MP + Accum, No Dropout) ---\")\n",
        "\n",
        "class TrainStateWithAccum(train_state.TrainState):\n",
        "    accum_grads: flax.core.FrozenDict\n",
        "\n",
        "\n",
        "learning_rate = 1e-4\n",
        "tx = optax.adamw(learning_rate=learning_rate, weight_decay=0.1)\n",
        "\n",
        "\n",
        "model_key, params_key = random.split(random.PRNGKey(123), 2)\n",
        "model = GPT()\n",
        "dummy_input = jnp.ones((1, config[\"context_length\"]), dtype=jnp.int32)\n",
        "params = model.init(params_key, dummy_input)['params']\n",
        "\n",
        "\n",
        "print(\"Verifying parameter dtypes (should be float32):\")\n",
        "jax.tree_util.tree_map(lambda p: print(f\" - Param shape: {p.shape}, dtype: {p.dtype}\"), params)\n",
        "\n",
        "zero_grads = jax.tree_util.tree_map(\n",
        "    lambda p: jnp.zeros_like(p, dtype=config[\"param_dtype\"]), params\n",
        ")\n",
        "\n",
        "state = TrainStateWithAccum.create(\n",
        "    apply_fn=model.apply, params=params, tx=tx, accum_grads=zero_grads\n",
        ")\n",
        "state = jax.device_put(state, device)\n",
        "\n",
        "param_count = sum(p.size for p in jax.tree_util.tree_leaves(state.params))\n",
        "print(f\"\\nModel initialized with {param_count:,} parameters.\")\n",
        "print(f\"Accumulated gradients dtype: {jax.tree_util.tree_leaves(state.accum_grads)[0].dtype}\")\n",
        "\n",
        "@functools.partial(jax.jit)\n",
        "def cross_entropy_loss(logits, targets):\n",
        "    one_hot_targets = jax.nn.one_hot(targets, num_classes=logits.shape[-1])\n",
        "    log_softmax_logits = jax.nn.log_softmax(logits.astype(config[\"output_head_dtype\"]), axis=-1)\n",
        "    loss_per_position = -jnp.sum(one_hot_targets * log_softmax_logits, axis=-1)\n",
        "    return jnp.mean(loss_per_position)\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, static_argnames=['model_apply'])\n",
        "def compute_grads_step(state, batch, model_apply):\n",
        "    x, y = batch\n",
        "    def compute_loss(params):\n",
        "\n",
        "        logits = model_apply({'params': params}, x)\n",
        "        loss = cross_entropy_loss(logits, y)\n",
        "        return loss\n",
        "    grad_fn = jax.value_and_grad(compute_loss)\n",
        "    loss, grads = grad_fn(state.params)\n",
        "    metrics = {'loss': loss}\n",
        "    return grads, metrics\n",
        "\n",
        "@functools.partial(jax.jit, static_argnames=['learning_rate_fn'])\n",
        "def apply_grads_step(state, accumulated_grads, learning_rate_fn):\n",
        "    avg_grads = jax.tree_util.tree_map(lambda g: g / config[\"gradient_accumulation_steps\"], accumulated_grads)\n",
        "    new_state = state.apply_gradients(grads=avg_grads)\n",
        "    zero_grads = jax.tree_util.tree_map(jnp.zeros_like, state.params)\n",
        "    new_state = new_state.replace(accum_grads=zero_grads)\n",
        "    lr = learning_rate_fn(state.step)\n",
        "    metrics = {'learning_rate': lr}\n",
        "    return new_state, metrics\n",
        "\n",
        "@functools.partial(jax.jit, static_argnames=['model_apply'])\n",
        "def eval_step(state, batch, model_apply):\n",
        "    x, y = batch\n",
        "    logits = model_apply({'params': state.params}, x)\n",
        "    loss = cross_entropy_loss(logits, y)\n",
        "    return {'loss': loss}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpQ41EgN1qle",
        "outputId": "5fc03294-3f5c-4828-8438-2fc341d4bb21"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Section 6: Training Setup (Manual MP + Accum, No Dropout) ---\n",
            "Verifying parameter dtypes (should be float32):\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (50257, 128), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128, 50257), dtype: float32\n",
            " - Param shape: (512,), dtype: float32\n",
            " - Param shape: (128, 512), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (512, 128), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (512,), dtype: float32\n",
            " - Param shape: (128, 512), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (512, 128), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (512,), dtype: float32\n",
            " - Param shape: (128, 512), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (512, 128), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (512,), dtype: float32\n",
            " - Param shape: (128, 512), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (512, 128), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (512,), dtype: float32\n",
            " - Param shape: (128, 512), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (512, 128), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (512,), dtype: float32\n",
            " - Param shape: (128, 512), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (512, 128), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128,), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            " - Param shape: (128, 128), dtype: float32\n",
            "\n",
            "Model initialized with 14,068,992 parameters.\n",
            "Accumulated gradients dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Section 7: Training Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Section 7: Training Loop (Manual MP + Accum + Remat) ---\")\n",
        "\n",
        "num_epochs = 1\n",
        "eval_frequency = 200\n",
        "accumulation_steps = config[\"gradient_accumulation_steps\"]\n",
        "\n",
        "train_key = random.PRNGKey(42)\n",
        "\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "print(f\"Compute dtype: {config['compute_dtype']}, Param dtype: {config['param_dtype']}\")\n",
        "print(f\"Gradient accumulation steps: {accumulation_steps}\")\n",
        "print(f\"Evaluation frequency (optimizer steps): {eval_frequency}\")\n",
        "print(\"Gradient Checkpointing (Rematerialization) is ENABLED for Transformer Blocks.\")\n",
        "print(\"Dropout is DISABLED.\")\n",
        "\n",
        "global_step_counter = 0\n",
        "optimizer_step_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n-- Epoch {epoch+1}/{num_epochs} --\")\n",
        "    epoch_key, train_key = random.split(train_key)\n",
        "    batch_generator = create_batches(\n",
        "        train_data, config[\"batch_size\"], config[\"context_length\"], epoch_key\n",
        "    )\n",
        "    num_sequences = len(train_data) - config[\"context_length\"]\n",
        "    total_micro_batches_per_epoch = num_sequences // config[\"batch_size\"]\n",
        "\n",
        "    pbar = tqdm(\n",
        "        enumerate(batch_generator), total=total_micro_batches_per_epoch,\n",
        "        desc=f\"Epoch {epoch+1} Training (Micro-batches)\"\n",
        "    )\n",
        "\n",
        "    interval_loss = 0.0\n",
        "    num_loss_samples = 0\n",
        "\n",
        "    for step, train_batch in pbar:\n",
        "        global_step_counter += 1\n",
        "        train_batch = jax.device_put(train_batch, device)\n",
        "        grads, compute_metrics = compute_grads_step(\n",
        "            state, train_batch, model.apply\n",
        "        )\n",
        "\n",
        "        state = state.replace(\n",
        "            accum_grads=jax.tree_util.tree_map(lambda acc, new: acc + new, state.accum_grads, grads)\n",
        "        )\n",
        "\n",
        "        interval_loss += compute_metrics['loss']\n",
        "        num_loss_samples += 1\n",
        "\n",
        "        if global_step_counter % accumulation_steps == 0:\n",
        "            optimizer_step_counter += 1\n",
        "\n",
        "            new_state, apply_metrics = apply_grads_step(\n",
        "                state, state.accum_grads, lambda step: learning_rate\n",
        "            )\n",
        "            state = new_state\n",
        "\n",
        "            if optimizer_step_counter % eval_frequency == 0:\n",
        "                avg_train_loss = jax.device_get(interval_loss / num_loss_samples) if num_loss_samples > 0 else 0.0\n",
        "                interval_loss, num_loss_samples = 0.0, 0\n",
        "\n",
        "                val_loss = 0.0\n",
        "                num_val_batches = 0\n",
        "                val_key, train_key = random.split(train_key)\n",
        "                val_batch_generator = create_batches(\n",
        "                    val_data, config[\"batch_size\"], config[\"context_length\"], val_key\n",
        "                )\n",
        "                for val_batch in val_batch_generator:\n",
        "                    val_batch = jax.device_put(val_batch, device)\n",
        "                    eval_metrics = eval_step(state, val_batch, model.apply)\n",
        "                    val_loss += eval_metrics['loss']\n",
        "                    num_val_batches += 1\n",
        "\n",
        "                avg_val_loss = jax.device_get(val_loss / num_val_batches) if num_val_batches > 0 else 0.0\n",
        "\n",
        "                pbar.set_postfix(OptStep=optimizer_step_counter, TrainLoss=f\"{avg_train_loss:.4f}\", ValLoss=f\"{avg_val_loss:.4f}\", LR=f\"{apply_metrics['learning_rate']:.1e}\")\n",
        "\n",
        "    pbar.close()\n",
        "    print(f\"-- Epoch {epoch+1} finished --\")\n",
        "\n",
        "print(\"\\nTraining complete.\")"
      ],
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Section 7: Training Loop (Manual MP + Accum + Remat, No Dropout) ---\n",
            "Starting training for 1 epochs...\n",
            "Compute dtype: <class 'jax.numpy.bfloat16'>, Param dtype: <class 'jax.numpy.float32'>\n",
            "Gradient accumulation steps: 8\n",
            "Evaluation frequency (optimizer steps): 200\n",
            "Gradient Checkpointing (Rematerialization) is ENABLED for Transformer Blocks.\n",
            "Dropout is DISABLED.\n",
            "\n",
            "-- Epoch 1/1 --\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "442f4bd78a764d1883388adfc8c7dca5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1 Training (Micro-batches):   0%|          | 0/281 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1 finished --\n",
            "\n",
            "Training complete.\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "442f4bd78a764d1883388adfc8c7dca5",
            "c2b082441b4745d3a7434edd84391038",
            "d215be99c3e24f0cad6dc9a9328af50a",
            "17aaf59ec43842d78d6c58e4a1355350",
            "865ae284780b4bf585496d2fb0e3626c",
            "cca62fdd6af143de92d07ae2e2516995",
            "53f7129d0737408893e4363a912f971c",
            "38e45b898b694a82a6a09a8c9ae52b4d",
            "04ec185b323c469082f108f63dff0a45",
            "79438c45a2de494286826133e33a88f2",
            "7c0a24a2e1554b2fbc4fd6bf0011834f"
          ]
        },
        "id": "YT3mR4w37CL-",
        "outputId": "05a0804f-f75b-461b-eaa9-a82398ed3e25"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "442f4bd78a764d1883388adfc8c7dca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2b082441b4745d3a7434edd84391038",
              "IPY_MODEL_d215be99c3e24f0cad6dc9a9328af50a",
              "IPY_MODEL_17aaf59ec43842d78d6c58e4a1355350"
            ],
            "layout": "IPY_MODEL_865ae284780b4bf585496d2fb0e3626c"
          }
        },
        "c2b082441b4745d3a7434edd84391038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cca62fdd6af143de92d07ae2e2516995",
            "placeholder": "​",
            "style": "IPY_MODEL_53f7129d0737408893e4363a912f971c",
            "value": "Epoch 1 Training (Micro-batches): 100%"
          }
        },
        "d215be99c3e24f0cad6dc9a9328af50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38e45b898b694a82a6a09a8c9ae52b4d",
            "max": 281,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04ec185b323c469082f108f63dff0a45",
            "value": 281
          }
        },
        "17aaf59ec43842d78d6c58e4a1355350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79438c45a2de494286826133e33a88f2",
            "placeholder": "​",
            "style": "IPY_MODEL_7c0a24a2e1554b2fbc4fd6bf0011834f",
            "value": " 281/281 [01:45&lt;00:00,  1.93it/s]"
          }
        },
        "865ae284780b4bf585496d2fb0e3626c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cca62fdd6af143de92d07ae2e2516995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f7129d0737408893e4363a912f971c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38e45b898b694a82a6a09a8c9ae52b4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04ec185b323c469082f108f63dff0a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79438c45a2de494286826133e33a88f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c0a24a2e1554b2fbc4fd6bf0011834f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}